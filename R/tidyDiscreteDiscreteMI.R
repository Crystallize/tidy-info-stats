#' Calculate single mutual information score from multiclass groups in dplyr friendly manner.
#'
#' The purpose of this is to make it possible to calculate MI from tidy data. This is useful where you have a a data from that
#' represents a multi-class confusion matrix with unique combinations of inputs and probabilities for the co-occurrence and marginal probabilities 
#' already calculated. Typically this will be generated by the probabilitiesFromCooccurrence function.
#' 
#' @param df a dataframe containing one observation per row & minimally p_x1y1, p_x1, p_y1 columns (see probabilitiesFromCounts / probabilitiesFromCooccurrence)
#' @return the datatable with additional columns for MI
#' @import dplyr
#' @export
calculateMultiClassMI = function(df) {
  return(
    df %>% mutate(
      pmi_x1y1 = ifelse( p_x1y1==0, ifelse(p_x1==0 | p_y1==0, 0, NA), log(p_x1y1/(p_x1*p_y1)) ),
      I_xy = ifelse(p_x1y1==0|p_x1==0|p_y1==0, 0, p_x1y1*pmi_x1y1)
    ) %>% summarise(
      I = sum(I_xy, na.rm=TRUE), #+ifelse(adjust,mm_adjust,0),
      I_sd = NA,
      method = "Empirical" #ifelse(adjust,"Empirical MM","Empirical")
    )
  )
}

#' calculate mutual information between a discrete value (X) and a discrete value (Y)
#' 
#' @param df - may be grouped, in which case the value is interpreted as different types of continuous variable
#' @param groupXVars - the column(s) of the discrete value (X) quoted by vars(...)
#' @param groupYVars - the column(s) of the discrete value (Y) quoted by vars(...)
#' @param method - the method employed - valid options are "Empirical","MontgomerySmith","Compression","Histogram","Entropy","Grassberger"
#' @param ... - the other parameters are passed onto the implementations
#' @return a dataframe containing the disctinct values of the groups of df, and for each group a mutual information column (I). If df was not grouped this will be a single entry
#' @export
calculateDiscreteDiscreteMI =  function(df, groupXVars, groupYVars, method="Grassberger", ...) {
  switch (method,
          Empirical = calculateDiscreteDiscreteMI_Empirical(df, groupXVars, groupYVars, ...),
          MontgomerySmith = calculateDiscreteDiscreteMI_Entropy(df, groupXVars, groupYVars, entropyMethod="MontgomerySmith", ...),
          Grassberger = calculateDiscreteDiscreteMI_Entropy(df, groupXVars, groupYVars, entropyMethod="Grassberger", ...),
          Compression = calculateDiscreteDiscreteMI_Entropy(df, groupXVars, groupYVars, entropyMethod="Compression", ...),
          Histogram = calculateDiscreteDiscreteMI_Entropy(df, groupXVars, groupYVars, entropyMethod="Histogram", ...),
          Entropy = calculateDiscreteDiscreteMI_Entropy(df, groupXVars, groupYVars, ...),
          {stop(paste0(method," not a valid option"))}
  )
}

#' calculate mutual information between a discrete value (X) and a discrete value (Y)
#' 
#' @param df - may be grouped, in which case the grouping is interpreted as different types of discrete variable
#' @param groupXVars - the column of the discrete value (X) quoted by vars(...)
#' @param groupYVars - the column of the discrete value (Y)
#' @return a dataframe containing the distinct values of the groups of df, and for each group a mutual information column (I). If df was not grouped this will be a single entry
#' @export
calculateDiscreteDiscreteMI_Empirical = function(df, groupXVars, groupYVars, ...) {
  df %>% probabilitiesFromCooccurrence(groupXVars, groupYVars) %>% calculateMultiClassMI()
}

#' calculate mutual information between a discrete value (X) and a discrete value (Y) using estimates of entropy
#' 
#' @param df - may be grouped, in which case the grouping is interpreted as different types of discrete variable
#' @param groupXVars - the column of the discrete value (X) quoted by vars(...)
#' @param groupYVars - the column of the discrete value (Y)
#' @param entropyMethod - the method used to calculate the entropy (see ?tidyinfostats::calculateEntropy) - defaults to "Grassberger"
#' @return a dataframe containing the disctinct values of the groups of df, and for each group a mutual information column (I). If df was not grouped this will be a single entry
#' @export
calculateDiscreteDiscreteMI_Entropy = function(df, groupXVars, groupYVars, entropyMethod="Grassberger", ...) {
  grps = df %>% groups()
  joinList = df %>% joinList(defaultJoin = "join")
  # list of join variables for join by value
  groupJoinList = df %>% joinList(groupXVars)
  
  Hy = df %>% group_by(!!!grps) %>% calculateEntropy(groupYVars, method = entropyMethod, ...) %>% rename(Hy = I, Hy_sd = I_sd) %>% mutate(join = 1)
  
  Hygivenx_tmp = df %>% group_by(!!!grps, !!!groupXVars) %>% calculateEntropy(groupYVars, method = entropyMethod, ...) %>% rename(Hygivenx = I, Hygivenx_sd = I_sd) %>% mutate(join = 1)
  
  Px = df %>% group_by(!!!grps) %>% groupwiseCount(groupXVars, summarise = TRUE) %>% mutate(p_x=as.double(N_x)/N, join=1)
  
  suppressWarnings({
    Hygivenx = Hygivenx_tmp %>% left_join(Px, by=groupJoinList) %>% group_by(!!!grps) %>% summarise(
      Hygivenx = sum(Hygivenx*p_x,na.rm = TRUE), 
      Hygivenx_sd = max(Hygivenx_sd*p_x,na.rm = TRUE)
    ) %>% mutate(join = 1) # sometimes max term has no non NA values. In which case this is NAl, which is ok but generates a warning
  })
  
  #TODO: refactor this to get a pointwise MI based on Hy/Px - H
  
  tmp2 = Hy %>% left_join(Hygivenx, by=joinList) %>% mutate(
    I = Hy-Hygivenx, 
    I_sd = Hy_sd+Hygivenx_sd,
    method =  paste0("Entropy - ",entropyMethod)
    ) %>% select(!!!grps, I, I_sd, method)
  
  return(tmp2 %>% ungroup())
}


